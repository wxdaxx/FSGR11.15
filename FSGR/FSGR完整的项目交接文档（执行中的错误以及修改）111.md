cd ~/autodl-tmp/FSGR

cat > HANDOVER_DOCUMENT.md << 'ENDOC'
# FSGRé¡¹ç›®å®Œæ•´äº¤æ¥æ–‡æ¡£

**åˆ›å»ºæ—¶é—´**: 2025-11-09  
**é¡¹ç›®çŠ¶æ€**: è®­ç»ƒå·²å¯åŠ¨ï¼Œé‡åˆ°CUDA asserté”™è¯¯éœ€è¦è¯Šæ–­  
**å®Œæˆåº¦**: 95% - æ•°æ®å’Œä»£ç å°±ç»ªï¼Œæ­£åœ¨è§£å†³è®­ç»ƒç¨³å®šæ€§é—®é¢˜

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç›®å½•ç»“æ„è¯¦è§£](#ç›®å½•ç»“æ„è¯¦è§£)
3. [æ•°æ®æµç¨‹å›¾](#æ•°æ®æµç¨‹å›¾)
4. [å·²è§£å†³é—®é¢˜æ¸…å•](#å·²è§£å†³é—®é¢˜æ¸…å•)
5. [ä»£ç ä¿®æ”¹è®°å½•](#ä»£ç ä¿®æ”¹è®°å½•)
6. [å½“å‰é—®é¢˜ä¸è°ƒè¯•æ­¥éª¤](#å½“å‰é—®é¢˜ä¸è°ƒè¯•æ­¥éª¤)
7. [å¿«é€Ÿå¯åŠ¨æŒ‡å—](#å¿«é€Ÿå¯åŠ¨æŒ‡å—)
8. [å¸¸è§é—®é¢˜FAQ](#å¸¸è§é—®é¢˜faq)

---

## é¡¹ç›®æ¦‚è¿°

### ç›®æ ‡
å®ç°FSGR (Fine-grained Semantic-Guided Region) å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨COCOæ•°æ®é›†ä¸Šè®­ç»ƒå¹¶è¯„ä¼°ã€‚

### æŠ€æœ¯æ ˆ
- **æ·±åº¦å­¦ä¹ æ¡†æ¶**: PyTorch 1.13.1 + CUDA 11.7
- **è§†è§‰ç¼–ç å™¨**: CLIP ViT-B/16
- **è¯­ä¹‰ç›‘ç£**: MaskCLIP
- **æ•°æ®é›†**: COCO 2014 (train + val)
- **ç¯å¢ƒ**: Conda (m2release), Python 3.8

### ç›¸å…³è®ºæ–‡
1. FSGR: ä¸»æ¨¡å‹æ¶æ„
2. CLIP: è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹
3. MaskCLIP: å¯†é›†é¢„æµ‹ä»»åŠ¡çš„CLIPé€‚é…
4. Transformer: Attentionæœºåˆ¶

---

## ç›®å½•ç»“æ„è¯¦è§£
```
~/autodl-tmp/FSGR/
â”‚
â”œâ”€â”€ ğŸ“ datasets/                          # æ•°æ®é›†ç›®å½•
â”‚   â””â”€â”€ coco/
â”‚       â””â”€â”€ images/
â”‚           â”œâ”€â”€ train2014/                # 82,783å¼ è®­ç»ƒå›¾åƒ
â”‚           â””â”€â”€ val2014/                  # 40,504å¼ éªŒè¯å›¾åƒ
â”‚
â”œâ”€â”€ ğŸ“ m2_annotations/                    # æ ‡æ³¨æ–‡ä»¶
â”‚   â”œâ”€â”€ coco_train_ids.npy              # è®­ç»ƒé›†annotation IDs
â”‚   â”œâ”€â”€ coco_dev_ids.npy                # éªŒè¯é›†annotation IDs
â”‚   â”œâ”€â”€ captions_train2014.json         # è®­ç»ƒé›†æè¿°
â”‚   â””â”€â”€ captions_val2014.json           # éªŒè¯é›†æè¿°
â”‚   è¯´æ˜: å·²ä¿®å¤image_idâ†’annotation_idæ˜ å°„
â”‚
â”œâ”€â”€ ğŸ“ .cache/clip/                       # CLIPé¢„è®­ç»ƒæ¨¡å‹
â”‚   â””â”€â”€ ViT-B-16.pt                     # CLIP ViT-B/16æƒé‡
â”‚
â”œâ”€â”€ ğŸ“ text_embeddings/                   # æ–‡æœ¬åµŒå…¥
â”‚   â””â”€â”€ ram_ViT16_clip_text.pth         # 80ä¸ªCOCOç±»åˆ«çš„CLIPæ–‡æœ¬ç‰¹å¾
â”‚
â”œâ”€â”€ ğŸ“„ word_embeds.pth                   # è¯åµŒå…¥ (10,201è¯)
â”œâ”€â”€ ğŸ“„ vocab.pkl                         # è¯è¡¨æ–‡ä»¶
â”‚
â”œâ”€â”€ ğŸ“ models/                            # æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ fsgr/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ transformer.py              # ä¸»Transformeræ¨¡å‹
â”‚       â”‚   ä¿®æ”¹: text_categories 4585â†’80
â”‚       â”‚   ä¿®æ”¹: æ‰€æœ‰.cuda()â†’.to(device)
â”‚       â”‚
â”‚       â”œâ”€â”€ encoders.py                 # TransformerEncoder
â”‚       â”‚   ä¿®æ”¹: .cuda()â†’.to(device)
â”‚       â”‚
â”‚       â”œâ”€â”€ decoders.py                 # TransformerDecoder
â”‚       â”‚   ä¿®æ”¹: æ·»åŠ _is_statefulåˆå§‹åŒ–
â”‚       â”‚   ä¿®æ”¹: .cuda()â†’.to(device)
â”‚       â”‚
â”‚       â”œâ”€â”€ projection.py               # MaskClipHead
â”‚       â”‚   ä¿®æ”¹: map_locationå…¼å®¹æ€§
â”‚       â”‚
â”‚       â”œâ”€â”€ attention.py                # æ³¨æ„åŠ›æœºåˆ¶
â”‚       â”œâ”€â”€ grid_aug.py                 # Grid augmentation
â”‚       â”‚   ä¿®æ”¹: .cuda()â†’.to(device)
â”‚       â”‚
â”‚       â””â”€â”€ optim_entry.py              # ä¼˜åŒ–å™¨å’ŒæŸå¤±
â”‚
â”œâ”€â”€ ğŸ“ data/                              # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py                      # COCOæ•°æ®é›†ç±»
â”‚   â”‚   é‡è¦: è¿”å›4å…ƒç´ batch
â”‚   â”‚
â”‚   â””â”€â”€ field.py                        # å­—æ®µå®šä¹‰
â”‚       â”œâ”€â”€ ImageDetectionsField        # å›¾åƒåŠ è½½
â”‚       â”‚   è¿”å›: (image_id, image_tensor, placeholder, ...)
â”‚       â””â”€â”€ TextField                   # æ–‡æœ¬å¤„ç†
â”‚
â”œâ”€â”€ ğŸ“ evaluation/                        # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cider/                          # CIDErè¯„åˆ†
â”‚   â”œâ”€â”€ bleu/                           # BLEUè¯„åˆ†
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“„ train_transformer.py              # ä¸»è®­ç»ƒè„šæœ¬ â­â­â­
â”‚   å…³é”®ä¿®æ”¹:
â”‚   1. batchè§£åŒ…: 4å…ƒç´  â†’ æ­£ç¡®ä½¿ç”¨images
â”‚   2. ç¦ç”¨CIDEr(éœ€è¦Java)
â”‚   3. æ·»åŠ é”™è¯¯å¤„ç†
â”‚   4. å‚æ•°ä¿®å¤: æ·»åŠ epochå‚æ•°e
â”‚
â”œâ”€â”€ ğŸ“„ train_transformer_working.py      # å·¥ä½œç‰ˆæœ¬å¤‡ä»½
â”œâ”€â”€ ğŸ“„ train_transformer_debug.py        # è°ƒè¯•ç‰ˆæœ¬(æ— é”™è¯¯æ•è·)
â”‚
â”œâ”€â”€ ğŸ“„ run_training_fixed.sh             # å¯åŠ¨è„šæœ¬ â­
â”‚
â”œâ”€â”€ ğŸ“ save_models/                       # æ¨¡å‹checkpoints
â”‚   â”œâ”€â”€ fsgr_baseline_test_last.pth
â”‚   â”œâ”€â”€ fsgr_baseline_test_best.pth
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ tensorboard_logs/                  # TensorBoardæ—¥å¿—
â”‚   â””â”€â”€ fsgr_baseline_test/
â”‚
â”œâ”€â”€ ğŸ“„ PROJECT_STATUS_COMPLETE.md        # å®Œæ•´è¿›åº¦æŠ¥å‘Š
â”œâ”€â”€ ğŸ“„ HANDOVER_DOCUMENT.md              # æœ¬æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ FINAL_STATUS_REPORT.md            # æŠ€æœ¯ç»†èŠ‚æŠ¥å‘Š
â”‚
â””â”€â”€ ğŸ“„ requirements.txt                   # Pythonä¾èµ–
```

---

## æ•°æ®æµç¨‹å›¾

### è®­ç»ƒæ•°æ®æµ
```
åŸå§‹å›¾åƒ (datasets/coco/images/train2014/*.jpg)
    â†“
ImageDetectionsField.preprocess()
    â†“ è¿”å› (image_id, image_tensor(3,224,224), random_placeholder, ...)
    â†“
Dataset.__getitem__()
    â†“ é…å¯¹å›¾åƒå’Œcaption
    â†“
DataLoader + collate_fn()
    â†“ batch = [image_ids, images, placeholders, captions]
    â†“
train_xe() - è®­ç»ƒå¾ªç¯
    â†“ å…³é”®ä¿®å¤: images_id, images, _, captions = batch
    â†“ ä½¿ç”¨images (è€Œéplaceholder)
    â†“
Transformer.forward(images, captions)
    â†“
â”œâ”€ CLIP Encoder â†’ Visual Features
â”œâ”€ TransformerEncoder â†’ Encoded Features
â””â”€ TransformerDecoder â†’ Generated Caption
    â†“
Loss Calculation (NLLLoss + SupConLoss)
    â†“
Backward + Optimizer.step()
```

### å…³é”®æ•°æ®å½¢çŠ¶
```python
# Batchç»“æ„
batch = [
    image_ids,      # torch.Size([32])           - å›¾åƒID
    images,         # torch.Size([32, 3, 224, 224]) - çœŸå®å›¾åƒ âœ“
    placeholder,    # torch.Size([32, 100, 2048])   - æœªä½¿ç”¨ âœ—
    captions        # torch.Size([32, seq_len])     - caption token IDs
]

# æ¨¡å‹è¾“å…¥
images: [batch_size, 3, 224, 224]          # RGBå›¾åƒ
captions: [batch_size, seq_len]            # Token indices (0 to vocab_size-1)

# æ¨¡å‹è¾“å‡º
output: [batch_size, seq_len, vocab_size]  # æ¯ä¸ªä½ç½®çš„è¯æ¦‚ç‡åˆ†å¸ƒ
```

---

## å·²è§£å†³é—®é¢˜æ¸…å•

### âœ… é—®é¢˜1: æ•°æ®é›†batchç»“æ„è¯¯è§£ (æœ€å…³é”®!)

**ç—‡çŠ¶**: 
- æ¨¡å‹è¾“å…¥ç»´åº¦é”™è¯¯
- DEBUGæ˜¾ç¤º input.shape = [32, 100, 2048] è€Œé [32, 3, 224, 224]

**æ ¹æœ¬åŸå› **:
```python
# Datasetå®é™…è¿”å›4ä¸ªå…ƒç´ 
batch = [image_ids, images, random_placeholder, captions]

# åŸä»£ç é”™è¯¯åœ°è§£åŒ…
detections, labels, captions = batch  # åªå–äº†3ä¸ªå…ƒç´ !
# å¯¼è‡´detections = images (æ­£ç¡®)
# ä½†åœ¨æœ‰äº›åœ°æ–¹detections = placeholder (é”™è¯¯!)
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¿®å¤åçš„æ­£ç¡®è§£åŒ…
if len(batch) == 4:
    images_id, images, _, captions = batch  # æ˜ç¡®å¿½ç•¥placeholder
    
out = model(images, captions)  # ä½¿ç”¨çœŸå®å›¾åƒ
```

**å½±å“**: è¿™æ˜¯è®­ç»ƒèƒ½å¦æ”¶æ•›çš„å…³é”®!å¦åˆ™æ¨¡å‹ä¼šç”¨éšæœºå™ªå£°è®­ç»ƒã€‚

---

### âœ… é—®é¢˜2: CUDAåˆå§‹åŒ–ä¸ç¨³å®š

**ç—‡çŠ¶**:
```
RuntimeError: No CUDA GPUs are available
```
å‡ºç°åœ¨: `model.to(device)` æˆ– `tensor.to(device)`

**å°è¯•è¿‡çš„æ–¹æ¡ˆ**:
1. âŒ ç¯å¢ƒå˜é‡è®¾ç½®
2. âŒ æ‰‹åŠ¨torch.cuda.init()
3. âŒ é™çº§PyTorchç‰ˆæœ¬
4. âŒ å»¶è¿Ÿ.to(device)è°ƒç”¨
5. âœ… å…‹éš†åˆ°æ–°GPUæœåŠ¡å™¨ - é—®é¢˜æ¶ˆå¤±

**ç»“è®º**: AutoDLç‰¹å®šç¯å¢ƒbugï¼Œæ¢æœåŠ¡å™¨è§£å†³ã€‚

---

### âœ… é—®é¢˜3: text_categorieså‚æ•°ä¸åŒ¹é…

**ç—‡çŠ¶**:
```
RuntimeError: The expanded size (4585) must match (80)
```

**åŸå› **:
- ä»£ç æœŸæœ›: 4585ä¸ªç±»åˆ« (MaskCLIPçš„å®Œæ•´æ¦‚å¿µåº“)
- å®é™…æä¾›: 80ä¸ªç±»åˆ« (COCOæ•°æ®é›†)

**è§£å†³**:
```python
# models/fsgr/transformer.py
# ä¿®æ”¹å‰
self.text_embed = MaskClipHead(text_categories=4585, ...)

# ä¿®æ”¹å  
self.text_embed = MaskClipHead(text_categories=80, ...)
```

---

### âœ… é—®é¢˜4: CIDErè¯„ä¼°éœ€è¦Java

**ç—‡çŠ¶**:
```
FileNotFoundError: [Errno 2] No such file or directory: 'java'
```

**è§£å†³**:
```python
# train_transformer.py
# è®­ç»ƒé˜¶æ®µç¦ç”¨CIDEr
cider_train = None  # åŸæœ¬: Cider(PTBTokenizer.tokenize(...))
cider_val = None
```

**è¯´æ˜**: CIDEråªåœ¨æœ€ç»ˆè¯„ä¼°æ—¶éœ€è¦ï¼Œè®­ç»ƒæ—¶å¯ä»¥ç¦ç”¨ã€‚

---

### âœ… é—®é¢˜5: PyTorchç‰ˆæœ¬å…¼å®¹æ€§

**ç—‡çŠ¶**:
- CUDA 13.0é©±åŠ¨ vs PyTorch CUDA 11.7
- å„ç§importåCUDAå¤±æ•ˆ

**è§£å†³**:
```bash
# é™çº§åˆ°æ›´ç¨³å®šçš„ç‰ˆæœ¬
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117
```

---

### âœ… é—®é¢˜6: æ–‡æœ¬åµŒå…¥åŠ è½½é”™è¯¯

**ç—‡çŠ¶**:
```
RuntimeError: Attempting to deserialize object on CUDA device but torch.cuda.is_available() is False
```

**åŸå› **: æ–‡æœ¬åµŒå…¥ç”¨æ—§PyTorchä¿å­˜ï¼Œæ–°ç‰ˆæœ¬åŠ è½½æ—¶CUDAä¸å…¼å®¹

**è§£å†³**:
```python
# models/fsgr/projection.py
# ä¿®æ”¹ä¸ºå…ˆåŠ è½½åˆ°CPU
loaded = torch.load(path, map_location='cpu')

# é‡æ–°ç”Ÿæˆæ–‡æœ¬åµŒå…¥
import clip
model, _ = clip.load("ViT-B/16", device="cuda")
# ... ç”Ÿæˆå¹¶ä¿å­˜
```

---

### âœ… é—®é¢˜7: Decoderä¸­_is_statefulæœªåˆå§‹åŒ–

**ç—‡çŠ¶**:
```
AttributeError: '_is_stateful' not initialized
```

**è§£å†³**:
```python
# models/fsgr/decoders.py - TransformerDecoderLayer.__init__
super(TransformerDecoderLayer, self).__init__()
self._is_stateful = False  # æ·»åŠ è¿™ä¸€è¡Œ
```

---

## ä»£ç ä¿®æ”¹è®°å½•

### æ–‡ä»¶: `train_transformer.py`

#### ä¿®æ”¹1: Batchè§£åŒ… (ç¬¬103-111è¡Œ)
```python
# ä¿®æ”¹å‰
for it, (detections, labels, captions) in enumerate(dataloader):
    detections, labels, captions = detections.to(device), labels.to(device), captions.to(device)

# ä¿®æ”¹å
for it, batch in enumerate(dataloader):
    if len(batch) == 4:
        images_id, images, _, captions = batch
    else:
        raise ValueError(f"Unexpected batch length: {len(batch)}")
    
    images = images.to(device)
    captions = captions.to(device)
```

**åŸå› **: Datasetè¿”å›4ä¸ªå…ƒç´ ï¼Œéœ€è¦æ­£ç¡®è§£åŒ…å¹¶ä½¿ç”¨çœŸå®å›¾åƒã€‚

---

#### ä¿®æ”¹2: æ¨¡å‹è°ƒç”¨ (ç¬¬118è¡Œ)
```python
# ä¿®æ”¹å‰
out = model(detections, captions)

# ä¿®æ”¹å
out = model(images, captions)
```

---

#### ä¿®æ”¹3: ç¦ç”¨CIDEr (ç¬¬250-251è¡Œ)
```python
# ä¿®æ”¹å‰
cider_train = Cider(PTBTokenizer.tokenize(ref_caps_train))
cider_val = Cider(PTBTokenizer.tokenize(ref_caps_val))

# ä¿®æ”¹å
cider_train = None
cider_val = None
```

---

#### ä¿®æ”¹4: æ·»åŠ é”™è¯¯å¤„ç† (ç¬¬111-136è¡Œ)
```python
try:
    # ... è®­ç»ƒä»£ç 
    with torch.cuda.amp.autocast():
        out = model(images, captions)
        # ... æŸå¤±è®¡ç®—
    
    optim.zero_grad()
    scaler.scale(loss).backward()
    scaler.step(optim)
    scaler.update()
    
    running_loss += loss.item()
    
except RuntimeError as e:
    error_msg = str(e).lower()
    if "assert" in error_msg:
        print(f"\nâš  Batch {it} å‡ºé”™ï¼Œè·³è¿‡: {str(e)[:100]}")
        continue
    raise
```

**è¯´æ˜**: æ•è·å¹¶è·³è¿‡é—®é¢˜batchï¼Œä½†è¿™ä¸ªæ–¹æ¡ˆæœ‰é—®é¢˜(è§å½“å‰é—®é¢˜)ã€‚

---

#### ä¿®æ”¹5: å‡½æ•°ç­¾å (ç¬¬99è¡Œ)
```python
# ä¿®æ”¹å‰
def train_xe(model, dataloader, optim, text_field, device, loss_contrast, beta=0.25):

# ä¿®æ”¹å
def train_xe(model, dataloader, optim, text_field, device, loss_contrast, e, beta=0.25):
```

**åŸå› **: å‡½æ•°å†…ä½¿ç”¨äº†epochå˜é‡eï¼Œä½†æœªä¼ å…¥ã€‚

---

### æ–‡ä»¶: `models/fsgr/transformer.py`

#### ä¿®æ”¹1: text_categories (ç¬¬43è¡Œ)
```python
# ä¿®æ”¹å‰
self.text_embed = MaskClipHead(text_categories=4585, ...)

# ä¿®æ”¹å
self.text_embed = MaskClipHead(text_categories=80, ...)
```

---

#### ä¿®æ”¹2: CUDAè°ƒç”¨ (ç¬¬53è¡ŒåŠå…¶ä»–)
```python
# ä¿®æ”¹å‰
self.backbone = build_model(...).cuda().float()

# ä¿®æ”¹å
self.backbone = build_model(...).to(device).float()
```

**åŸå› **: .cuda()åœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¤±è´¥ï¼Œ.to(device)æ›´å¯é ã€‚

---

### æ–‡ä»¶: `models/fsgr/decoders.py`

#### ä¿®æ”¹1: åˆå§‹åŒ–_is_stateful (ç¬¬86è¡Œ)
```python
# åœ¨__init__ä¸­æ·»åŠ 
super(TransformerDecoderLayer, self).__init__()
self._is_stateful = False  # æ·»åŠ æ­¤è¡Œ
```

---

#### ä¿®æ”¹2: CUDAè°ƒç”¨
```python
# æ‰€æœ‰.cuda()æ”¹ä¸º.to(device)
# ä¾‹å¦‚ç¬¬127è¡Œ
mask_self_attention = torch.zeros(...).to(device)
```

---

### æ–‡ä»¶: `models/fsgr/projection.py`

#### ä¿®æ”¹: map_location (ç¬¬35è¡Œ)
```python
# ä¿®æ”¹å‰
loaded = torch.load(self.text_embeddings_path, map_location='cuda')

# ä¿®æ”¹å
loaded = torch.load(self.text_embeddings_path, map_location='cpu')
```

---

### æ–‡ä»¶: `models/fsgr/encoders.py`, `grid_aug.py`

#### ä¿®æ”¹: æ‰€æœ‰CUDAè°ƒç”¨
```python
# æ·»åŠ deviceå®šä¹‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ‰€æœ‰.cuda()æ”¹ä¸º.to(device)
```

---

## å½“å‰é—®é¢˜ä¸è°ƒè¯•æ­¥éª¤

### ğŸ”´ å½“å‰é—®é¢˜: CUDA device-side assert

**ç°è±¡**:
```
ä»batch 1135å¼€å§‹æŒç»­å‡ºé”™:
âš  Batch 1135 å‡ºé”™,è·³è¿‡: CUDA error: device-side assert triggered
âš  Batch 1136 å‡ºé”™,è·³è¿‡: CUDA error: device-side assert triggered
... æŒç»­åˆ°batch 1530+
```

**åˆ†æ**:
1. å‰1134ä¸ªbatchè®­ç»ƒæ­£å¸¸ âœ“
2. ç¬¬1135ä¸ªbatchè§¦å‘assert
3. CUDA contextè¢«ç ´åï¼Œåç»­æ‰€æœ‰æ“ä½œå¤±è´¥ âœ—

**é—®é¢˜**: ç®€å•è·³è¿‡ä¸å¤Ÿï¼Œå› ä¸ºCUDAçŠ¶æ€å·²è¢«æ±¡æŸ“ã€‚

---

### ğŸ” éœ€è¦è¯Šæ–­çš„æ–¹å‘

#### æ–¹å‘1: Token indexè¶…å‡ºèŒƒå›´ â­â­â­â­â­
**æœ€å¯èƒ½çš„åŸå› **

æ£€æŸ¥è„šæœ¬:
```bash
python << 'EOF'
import pickle
from data import TextField, COCO, ImageDetectionsField, DataLoader

# åŠ è½½vocab
with open('vocab.pkl', 'rb') as f:
    vocab = pickle.load(f)

vocab_size = len(vocab)
print(f"Vocabå¤§å°: {vocab_size}")

# åˆ›å»ºdataset
image_field = ImageDetectionsField(
    detections_path='datasets/coco/images/train2014/COCO_train2014_000000000009.jpg',
    load_in_tmp=False
)
text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy', remove_punctuation=True, nopoints=False)
text_field.vocab = vocab

dataset = COCO(image_field, text_field, 'datasets/coco/images/', 'm2_annotations', 'm2_annotations')
train_dataset, _, _ = dataset.splits

# æ£€æŸ¥å‰1200ä¸ªbatch
loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=0)

problem_found = False
for i, batch in enumerate(loader):
    if i > 40:  # æ£€æŸ¥åˆ°batch 40 (è¦†ç›–1135çš„ä½ç½®)
        break
    
    _, _, _, captions = batch
    max_idx = captions.max().item()
    min_idx = captions.min().item()
    
    if max_idx >= vocab_size or min_idx < 0:
        print(f"âœ— Batch {i}: æ— æ•ˆtoken! max={max_idx}, min={min_idx}, vocab_size={vocab_size}")
        problem_found = True
        break
    
    if i % 10 == 0:
        print(f"âœ“ Batch {i}: OK (max={max_idx}, min={min_idx}, vocab={vocab_size})")

if not problem_found:
    print("\nâœ“ å‰40ä¸ªbatchçš„tokenséƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…")
EOF
```

#### æ–¹å‘2: NaNå€¼ä¼ æ’­
```bash
# åœ¨æ¨¡å‹forwardå‰æ·»åŠ æ£€æŸ¥
if torch.isnan(images).any():
    print(f"âœ— Batch {it}: imagesåŒ…å«NaN")
    continue
if torch.isinf(images).any():
    print(f"âœ— Batch {it}: imagesåŒ…å«Inf")
    continue
```

#### æ–¹å‘3: Captioné•¿åº¦é—®é¢˜
```bash
# æ£€æŸ¥æ˜¯å¦æœ‰è¶…é•¿caption
if captions.shape[1] > 54:
    print(f"âœ— Batch {it}: captionå¤ªé•¿ {captions.shape[1]} > 54")
    continue
```

---

### ğŸ”§ å»ºè®®çš„ä¿®å¤æ­¥éª¤

#### æ­¥éª¤1: è¯Šæ–­çœŸæ­£åŸå› 
```bash
cd ~/autodl-tmp/FSGR

# è¿è¡Œvocabæ£€æŸ¥
python check_vocab.py

# è¿è¡Œè°ƒè¯•ç‰ˆæœ¬(ä¸è·³è¿‡é”™è¯¯)
export CUDA_LAUNCH_BLOCKING=1
python train_transformer_debug.py \
  --text \
  --batch_size 32 \
  --workers 0 \
  --features_path datasets/coco/images \
  --annotation_folder m2_annotations \
  ... 2>&1 | tee error_log.txt
```

#### æ­¥éª¤2: æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤
å¦‚æœæ˜¯vocabé—®é¢˜:
```python
# åœ¨train_xeä¸­æ·»åŠ 
vocab_size = len(text_field.vocab)
if captions.max() >= vocab_size:
    print(f"Skipping batch {it}: invalid tokens")
    continue
```

å¦‚æœæ˜¯NaNé—®é¢˜:
```python
# æ·»åŠ NaNæ£€æµ‹
if torch.isnan(images).any() or torch.isnan(captions.float()).any():
    print(f"Skipping batch {it}: NaN detected")
    continue
```

#### æ­¥éª¤3: CUDAé”™è¯¯åé‡å¯
CUDA assertåï¼Œéœ€è¦é‡å¯Pythonè¿›ç¨‹:
```python
# æ–¹æ¡ˆA: æ•è·é”™è¯¯åé€€å‡ºï¼Œè®©å¤–éƒ¨è„šæœ¬é‡å¯
except RuntimeError as e:
    if "assert" in str(e):
        print(f"CUDA error at batch {it}, saving checkpoint and exiting...")
        torch.save(model.state_dict(), 'emergency_checkpoint.pth')
        sys.exit(1)

# æ–¹æ¡ˆB: å®šæœŸä¿å­˜checkpointï¼Œå‡ºé”™åä»checkpointæ¢å¤
```

---

## å¿«é€Ÿå¯åŠ¨æŒ‡å—

### é¦–æ¬¡è¿è¡Œ
```bash
# 1. æ¿€æ´»ç¯å¢ƒ
conda activate m2release

# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/autodl-tmp/FSGR

# 3. éªŒè¯æ•°æ®å®Œæ•´æ€§
ls datasets/coco/images/train2014/ | wc -l  # åº”è¯¥æ˜¯82783
ls datasets/coco/images/val2014/ | wc -l    # åº”è¯¥æ˜¯40504
ls vocab.pkl .cache/clip/ViT-B-16.pt         # åº”è¯¥éƒ½å­˜åœ¨

# 4. éªŒè¯ç¯å¢ƒ
python -c "import torch; print('CUDA:', torch.cuda.is_available())"

# 5. å¯åŠ¨è®­ç»ƒ
export CUDA_LAUNCH_BLOCKING=1
./run_training_fixed.sh
```

### ä»checkpointæ¢å¤
```bash
# ä¿®æ”¹run_training_fixed.shï¼Œæ·»åŠ 
--resume_last \

# ç„¶åè¿è¡Œ
./run_training_fixed.sh
```

### è°ƒè¯•æ¨¡å¼
```bash
# ä½¿ç”¨è°ƒè¯•ç‰ˆæœ¬(æ˜¾ç¤ºå®Œæ•´é”™è¯¯)
python train_transformer_debug.py \
  --text \
  --batch_size 32 \
  --workers 0 \
  --features_path datasets/coco/images \
  --annotation_folder m2_annotations \
  --text_embed_path text_embeddings/ram_ViT16_clip_text.pth \
  --pre_vs_path .cache/clip/ViT-B-16.pt \
  --pre_name "ViT-B/16" \
  --head 8 \
  --m 54
```

---

## å¸¸è§é—®é¢˜FAQ

### Q1: CUDAä¸å¯ç”¨æ€ä¹ˆåŠï¼Ÿ
```bash
# æ£€æŸ¥
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# å¦‚æœFalse
# 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $CUDA_VISIBLE_DEVICES  # åº”è¯¥æ˜¯0æˆ–ç©º

# 2. é‡æ–°å®‰è£…PyTorch
pip uninstall torch torchvision torchaudio -y
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 \
    --extra-index-url https://download.pytorch.org/whl/cu117
```

### Q2: OOM (Out of Memory)
```bash
# å‡å°batch size
--batch_size 16  # ä»32å‡åˆ°16

# æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--accumulation_steps 2
```

### Q3: è®­ç»ƒé€Ÿåº¦æ…¢
```bash
# å¢åŠ workers (å¦‚æœå†…å­˜è¶³å¤Ÿ)
--workers 2  # ä»0å¢åŠ åˆ°2

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (å·²å¯ç”¨)
# train_transformer.pyä¸­å·²æœ‰:
# with torch.cuda.amp.autocast():
```

### Q4: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼Ÿ
```bash
# TensorBoard
tensorboard --logdir=tensorboard_logs/fsgr_baseline_test --port 6006

# æŸ¥çœ‹æœ€æ–°checkpoint
ls -lht save_models/
```

### Q5: æ¨¡å‹checkpointåœ¨å“ªé‡Œï¼Ÿ
```bash
save_models/
â”œâ”€â”€ fsgr_baseline_test_last.pth    # æœ€æ–°çš„
â”œâ”€â”€ fsgr_baseline_test_best.pth    # éªŒè¯é›†æœ€ä½³
â””â”€â”€ fsgr_baseline_test_best_test.pth  # æµ‹è¯•é›†æœ€ä½³
```

---

## ä¸‹ä¸€æ­¥TODO

### ğŸ”´ ç´§æ€¥ (ç«‹å³å¤„ç†)
- [ ] è¯Šæ–­CUDA assertçš„çœŸæ­£åŸå› 
  - è¿è¡Œvocabæ£€æŸ¥è„šæœ¬
  - è¿è¡Œè°ƒè¯•ç‰ˆæœ¬è·å–å®Œæ•´é”™è¯¯
- [ ] æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤ä»£ç 
- [ ] éªŒè¯ä¿®å¤åèƒ½å¦ç¨³å®šè®­ç»ƒ

### ğŸŸ¡ é‡è¦ (æœ¬å‘¨å®Œæˆ)
- [ ] å®Œæˆè‡³å°‘1ä¸ªå®Œæ•´epochçš„è®­ç»ƒ
- [ ] éªŒè¯lossä¸‹é™è¶‹åŠ¿
- [ ] åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æŒ‡æ ‡

### ğŸŸ¢ é•¿æœŸ (1-2å‘¨)
- [ ] å®ŒæˆXEè®­ç»ƒé˜¶æ®µ(15-100 epochs)
- [ ] å¯é€‰: åˆ‡æ¢åˆ°RLè®­ç»ƒ
- [ ] è¾¾åˆ°è®ºæ–‡æŠ¥å‘Šçš„baselineæ€§èƒ½

### ğŸ”µ æ‰©å±• (æœªæ¥)
- [ ] ç†è§£FSGRå®Œæ•´æ¶æ„
- [ ] è®¾è®¡PMAèåˆæ–¹æ¡ˆ
- [ ] å®ç°åˆ›æ–°ç‚¹å¹¶éªŒè¯

---

## è”ç³»ä¸æ”¯æŒ

### å·²è§£å†³é—®é¢˜çš„å‚è€ƒ
- `PROJECT_STATUS_COMPLETE.md` - å®Œæ•´è¿›åº¦å’Œä¿®å¤è®°å½•
- `FINAL_STATUS_REPORT.md` - æŠ€æœ¯ç»†èŠ‚
- æœ¬æ–‡æ¡£ - é¡¹ç›®äº¤æ¥

### å¦‚éœ€å¸®åŠ©
1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„FAQéƒ¨åˆ†
2. æ£€æŸ¥å·²è§£å†³é—®é¢˜æ¸…å•
3. è¿è¡Œè¯Šæ–­è„šæœ¬æ”¶é›†ä¿¡æ¯
4. æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯å’Œç¯å¢ƒçŠ¶æ€

### é‡è¦æ–‡ä»¶å¤‡ä»½
```bash
# åˆ›å»ºå¤‡ä»½
tar -czf FSGR_backup_$(date +%Y%m%d).tar.gz \
  train_transformer_working.py \
  vocab.pkl \
  word_embeds.pth \
  text_embeddings/ \
  .cache/clip/ \
  *.md
```

---

## é™„å½•

### A. å®Œæ•´ä¾èµ–åˆ—è¡¨
```
torch==1.13.1+cu117
torchvision==0.14.1+cu117
torchaudio==0.13.1
timm==0.6.12
h5py==3.8.0
spacy==3.5.0
pycocotools==2.0.6
tqdm==4.65.0
tensorboard==2.12.0
Pillow==9.5.0
numpy==1.24.2
```

### B. ç›®å½•å¤§å°å‚è€ƒ
```
datasets/coco/images/train2014/  ~13GB
datasets/coco/images/val2014/    ~6GB
.cache/clip/                     ~350MB
save_models/                     ~2GB per checkpoint
```

### C. è®­ç»ƒæ—¶é—´ä¼°ç®—
```
ç¡¬ä»¶: A100 40GB
Batch size: 32
Workers: 0

1 epoch â‰ˆ 17,710 batches
é€Ÿåº¦: ~3.5 it/s
é¢„è®¡æ—¶é—´: ~1.5å°æ—¶/epoch

å®Œæ•´è®­ç»ƒ(100 epochs): ~6å¤©
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**æœ€åæ›´æ–°**: 2025-11-09  
**ç»´æŠ¤è€…**: [Your Name]  
**é¡¹ç›®çŠ¶æ€**: è®­ç»ƒå¯åŠ¨ä¸­ï¼Œæ­£åœ¨è§£å†³ç¨³å®šæ€§é—®é¢˜

---

**è¿™ä»½æ–‡æ¡£åº”è¯¥åŒ…å«äº†ä¸‹ä¸€ä½æ¥æ‰‹è€…éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ã€‚ç¥å¥½è¿ï¼** ğŸš€
ENDOC

cat HANDOVER_DOCUMENT.md

echo ""
echo "=========================================="
echo "âœ… å®Œæ•´äº¤æ¥æ–‡æ¡£å·²ç”Ÿæˆ"
echo "=========================================="
echo ""
echo "ğŸ“„ æ–‡ä»¶: ~/autodl-tmp/FSGR/HANDOVER_DOCUMENT.md"
echo "ğŸ“„ å¤§å°: $(wc -l HANDOVER_DOCUMENT.md | awk '{print $1}') è¡Œ"
echo ""
echo "ğŸ“‹ æ–‡æ¡£åŒ…å«:"
echo "  âœ“ å®Œæ•´ç›®å½•ç»“æ„å’Œæ–‡ä»¶è¯´æ˜"
echo "  âœ“ æ•°æ®æµç¨‹å›¾"
echo "  âœ“ æ‰€æœ‰å·²è§£å†³é—®é¢˜çš„è¯¦ç»†è®°å½•"
echo "  âœ“ æ¯ä¸ªæ–‡ä»¶çš„å…·ä½“ä¿®æ”¹(å¸¦è¡Œå·)"
echo "  âœ“ å½“å‰é—®é¢˜çš„è¯Šæ–­æ­¥éª¤"
echo "  âœ“ å¿«é€Ÿå¯åŠ¨æŒ‡å—"
echo "  âœ“ FAQå’Œtroubleshooting"
echo ""
echo "ğŸ¯ ç°åœ¨è¿è¡Œè¯Šæ–­è„šæœ¬:"
echo "=========================================="
