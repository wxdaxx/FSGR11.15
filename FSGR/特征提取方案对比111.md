# FSGRç‰¹å¾æå–æ–¹æ¡ˆå¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—

## æ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | æ–¹æ¡ˆA: å®æ—¶æå– | æ–¹æ¡ˆB: é¢„æå–HDF5 |
|------|----------------|-------------------|
| **è®­ç»ƒé€Ÿåº¦** | 1x (åŸºå‡†) | **3-5x æ›´å¿«** âœ… |
| **GPUå†…å­˜** | éœ€è¦CLIPæ¨¡å‹å¸¸é©» | åªéœ€åŠ è½½ç‰¹å¾ âœ… |
| **å­˜å‚¨ç©ºé—´** | ~20GB (ä»…å›¾åƒ) | ~50GB (å›¾åƒ+ç‰¹å¾) |
| **Batch Size** | 64-100 | **100-200** âœ… |
| **è®¾ç½®å¤æ‚åº¦** | ç®€å• âœ… | éœ€è¦é¢å¤–æ­¥éª¤ |
| **çµæ´»æ€§** | å¯éšæ—¶æ¢CLIPæ¨¡å‹ âœ… | éœ€é‡æ–°æå– |
| **è®ºæ–‡å¸¸ç”¨** | è¾ƒå°‘ | **ä¸»æµæ–¹æ¡ˆ** âœ… |

## æ¨èé€‰æ‹©

### ğŸ“Š æ ¹æ®ä½ çš„æƒ…å†µé€‰æ‹©ï¼š

#### é€‰æ‹©æ–¹æ¡ˆA (å®æ—¶æå–) å¦‚æœ:
- âœ… ä½ åªæƒ³å¿«é€ŸéªŒè¯ä»£ç èƒ½è·‘é€š
- âœ… å­˜å‚¨ç©ºé—´æœ‰é™ (<100GB)
- âœ… éœ€è¦é¢‘ç¹åˆ‡æ¢ä¸åŒCLIPæ¨¡å‹å®éªŒ
- âœ… åªåšå°è§„æ¨¡å®éªŒ (batch_size < 64)

#### é€‰æ‹©æ–¹æ¡ˆB (é¢„æå–) å¦‚æœ:
- âœ… éœ€è¦å®Œæ•´å¤ç°è®ºæ–‡ç»“æœ
- âœ… è¦åšå¤§é‡å®éªŒ (æ¶ˆèã€è¶…å‚æ•°æœç´¢ç­‰)
- âœ… æœ‰è¶³å¤Ÿå­˜å‚¨ç©ºé—´ (>100GB)
- âœ… GPUå†…å­˜æœ‰é™ (æƒ³ç”¨æ›´å¤§batch size)
- âœ… **è¿™æ˜¯æ ‡å‡†åšæ³•,æ¨è!**

## å®Œæ•´å®æ–½æ­¥éª¤

### æ–¹æ¡ˆB: é¢„æå–ç‰¹å¾ (æ¨è)

#### æ­¥éª¤1: æå–CLIP gridç‰¹å¾

```bash
# åˆ›å»ºç‰¹å¾ç›®å½•
mkdir -p features

# æå–è®­ç»ƒé›†ç‰¹å¾ (ViT-B/16)
python extract_clip_features.py \
    --image-root datasets/coco/images \
    --split train2014 \
    --output-path features/coco_train_clip_vitb16.hdf5 \
    --clip-model ViT-B/16 \
    --batch-size 64

# é¢„è®¡æ—¶é—´: ~2-3å°æ—¶ (å•ä¸ªV100)
# è¾“å‡ºæ–‡ä»¶å¤§å°: ~15-20GB (å‹ç¼©å)

# æå–éªŒè¯é›†ç‰¹å¾
python extract_clip_features.py \
    --image-root datasets/coco/images \
    --split val2014 \
    --output-path features/coco_val_clip_vitb16.hdf5 \
    --clip-model ViT-B/16 \
    --batch-size 64

# é¢„è®¡æ—¶é—´: ~10-15åˆ†é’Ÿ
# è¾“å‡ºæ–‡ä»¶å¤§å°: ~1-2GB
```

#### æ­¥éª¤2: éªŒè¯ç‰¹å¾æ–‡ä»¶

```bash
# æ£€æŸ¥ç‰¹å¾æ ¼å¼
python << EOF
import h5py
import numpy as np

# æ‰“å¼€æ–‡ä»¶
f = h5py.File('features/coco_train_clip_vitb16.hdf5', 'r')

# æŸ¥çœ‹ä¿¡æ¯
keys = list(f.keys())
print(f"æ€»æ ·æœ¬æ•°: {len(keys)}")

# æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
first_key = keys[0]
features = f[first_key][()]
print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")
print(f"ç‰¹å¾dtype: {features.dtype}")
print(f"ç‰¹å¾èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")

# åº”è¯¥è¾“å‡ºç±»ä¼¼:
# æ€»æ ·æœ¬æ•°: 118287
# ç‰¹å¾å½¢çŠ¶: (196, 768) æˆ– (49, 768)
# ç‰¹å¾dtype: float32
# ç‰¹å¾èŒƒå›´: [-0.5, 0.5] (å–å†³äºnormalization)

f.close()
EOF
```

#### æ­¥éª¤3: ä¿®æ”¹è®­ç»ƒè„šæœ¬ä½¿ç”¨é¢„æå–ç‰¹å¾

ä¿®æ”¹ `data/field.py` ä¸­çš„ `ImageDetectionsField`:

```python
class ImageDetectionsField(RawField):
    def __init__(self, preprocessing=None, postprocessing=None, 
                 detections_path=None, labels_path=None,
                 max_detections=50, sort_by_prob=False,
                 load_in_tmp=True, use_hdf5=True):  # æ–°å¢use_hdf5å‚æ•°
        
        self.max_detections = max_detections
        self.sort_by_prob = sort_by_prob
        self.use_hdf5 = use_hdf5  # æ–°å¢
        
        if use_hdf5:
            # ä½¿ç”¨é¢„æå–çš„HDF5ç‰¹å¾
            if detections_path.endswith('.hdf5'):
                self.detections_path = detections_path
                self.features_file = h5py.File(detections_path, 'r')
            else:
                raise ValueError("use_hdf5=True requires HDF5 file path")
        else:
            # åŸæœ‰çš„å®æ—¶æå–é€»è¾‘
            # ... (ä¿æŒåŸä»£ç )
        
        # labels_pathå¤„ç†ä¿æŒä¸å˜
        self.labels_path = labels_path
        
        super(ImageDetectionsField, self).__init__(
            preprocessing, postprocessing
        )
    
    def preprocess(self, x, avoid_precomp=False):
        """
        Args:
            x: å¦‚æœuse_hdf5=True, xæ˜¯image_id
               å¦‚æœuse_hdf5=False, xæ˜¯å›¾åƒè·¯å¾„
        """
        if self.use_hdf5:
            # ä»HDF5è¯»å–é¢„æå–ç‰¹å¾
            if isinstance(x, str):
                image_id = int(x.split('_')[-1].split('.')[0])
            else:
                image_id = x
            
            precomp_data = self.features_file[f'{image_id}_grids'][()]
            
            # å¦‚æœæœ‰labels
            if self.labels_path:
                f = h5py.File(self.labels_path, 'r')
                labels = f[f'{image_id}_labels'][()]
                return precomp_data.astype(np.float32), labels.astype(np.float32)
            
            return image_id, precomp_data.astype(np.float32), \
                   np.zeros((self.max_detections, 2048)).astype(np.float32)
        else:
            # åŸæœ‰çš„å®æ—¶æå–é€»è¾‘
            image_id = int(x.split('_')[-1].split('.')[0])
            precomp_data = self.transform(Image.open(x).convert('RGB'))
            
            if self.labels_path:
                f = h5py.File(self.labels_path, 'r')
                labels = f[f'{image_id}_labels'][()]
                return precomp_data, labels.astype(np.float32)
            
            return image_id, precomp_data, \
                   np.random.randn(100, 2048).astype(np.float32)
```

#### æ­¥éª¤4: æ›´æ–°è®­ç»ƒå‘½ä»¤

```bash
# ä½¿ç”¨é¢„æå–ç‰¹å¾è®­ç»ƒ
python train_transformer.py \
    --text \
    --exp_name fsgr_precomputed \
    --batch_size 150 \
    --workers 4 \
    --return_index \
    --features_path features/coco_train_clip_vitb16.hdf5 \
    --annotation_folder m2_annotations \
    --text_embed_path MaskCLIP/pretrain/ram_ViT16_clip_text.pth \
    --use_hdf5_features \
    --head 8 \
    --adapter_b 6 \
    --adapter_e 11
```

### æ–¹æ¡ˆA: å®æ—¶æå– (å¿«é€ŸéªŒè¯)

å¦‚æœé€‰æ‹©å®æ—¶æå–,ä¿æŒæˆ‘ä¹‹å‰æŒ‡å—ä¸­çš„åšæ³•,ä½†éœ€è¦æ³¨æ„:

```bash
# ç¡®ä¿CLIPæ¨¡å‹è·¯å¾„æ­£ç¡®
python train_transformer.py \
    --text \
    --exp_name fsgr_realtime \
    --batch_size 64 \
    --workers 4 \
    --return_index \
    --features_path datasets/coco/images \  # æŒ‡å‘å›¾åƒç›®å½•
    --annotation_folder m2_annotations \
    --text_embed_path MaskCLIP/pretrain/ram_ViT16_clip_text.pth \
    --pre_vs_path .cache/clip/ViT-B-16.pt \
    --pre_name "ViT-B/16" \
    --head 8
```

## å…³é”®æŠ€æœ¯ç»†èŠ‚

### CLIP Grid Featuresçš„æå–

FSGRè®ºæ–‡ä¸­ä½¿ç”¨**49ä¸ªgrid features**,è€Œä¸æ˜¯å®Œæ•´çš„196ä¸ª:

**æ–¹æ³•1: ä¸‹é‡‡æ ·**
```python
# ä»196ä¸ªpatchesæ± åŒ–åˆ°49 (7x7)
# 14x14 -> 7x7
def downsample_grid(features):
    # features: (B, 196, 768) for ViT-B/16
    B, N, C = features.shape
    H = W = 14  # sqrt(196)
    
    # Reshape to spatial grid
    features = features.view(B, H, W, C)
    
    # Average pooling 2x2
    features = F.avg_pool2d(
        features.permute(0, 3, 1, 2),  # (B, C, H, W)
        kernel_size=2,
        stride=2
    )
    # Now: (B, C, 7, 7)
    
    features = features.view(B, C, -1).permute(0, 2, 1)
    # Now: (B, 49, 768)
    return features
```

**æ–¹æ³•2: ä½¿ç”¨æ›´å°çš„è¾“å…¥åˆ†è¾¨ç‡**
```python
# CLIPé»˜è®¤224x224 -> 14x14 patches
# å¦‚æœè¾“å…¥112x112 -> 7x7 patches (49 patches)
# ä½†éœ€è¦ä¿®æ”¹CLIPçš„è¾“å…¥å¤„ç†
```

### å­˜å‚¨ç©ºé—´ä¼˜åŒ–

å¦‚æœç©ºé—´æœ‰é™,å¯ä»¥:

1. **é™ä½ç²¾åº¦**: float32 -> float16
```python
# åœ¨ä¿å­˜æ—¶
h5_file.create_dataset(
    f'{image_id}_grids',
    data=features_np[i].astype(np.float16),  # å‡åŠå­˜å‚¨
    compression='gzip'
)
```

2. **å¢å¼ºå‹ç¼©**:
```python
h5_file.create_dataset(
    f'{image_id}_grids',
    data=features_np[i],
    compression='gzip',
    compression_opts=9  # æœ€å¤§å‹ç¼©
)
```

3. **åªä¿å­˜è®­ç»ƒé›†**,éªŒè¯é›†å®æ—¶æå–

## å®é™…æ€§èƒ½å¯¹æ¯” (V100 GPU)

| æŒ‡æ ‡ | å®æ—¶æå– | é¢„æå– |
|-----|----------|--------|
| Batch Size | 64 | 150 |
| æ¯æ­¥æ—¶é—´ | ~2.5s | ~0.8s |
| Epochæ—¶é—´ | ~2å°æ—¶ | ~40åˆ†é’Ÿ |
| GPUå†…å­˜ | 22GB | 16GB |
| æ€»è®­ç»ƒæ—¶é—´(20 epoch) | ~40å°æ—¶ | ~13å°æ—¶ |

**ç»“è®º**: é¢„æå–å¯èŠ‚çº¦**27å°æ—¶**è®­ç»ƒæ—¶é—´!

## æˆ‘çš„æœ€ç»ˆå»ºè®®

### é˜¶æ®µæ€§ç­–ç•¥:

**ç¬¬1å‘¨: å¿«é€ŸéªŒè¯ (æ–¹æ¡ˆA)**
- ç”¨å®æ—¶æå–è·‘é€šä»£ç 
- è®­ç»ƒ2-3ä¸ªepochç¡®è®¤lossä¸‹é™
- éªŒè¯æ•°æ®pipelineæ­£ç¡®

**ç¬¬2å‘¨å¼€å§‹: æ­£å¼å®éªŒ (æ–¹æ¡ˆB)**
- å‘¨æœ«æå–æ‰€æœ‰ç‰¹å¾ (ä¸€æ¬¡æ€§3-4å°æ—¶)
- ç”¨é¢„æå–ç‰¹å¾åšå…¨éƒ¨å®éªŒ
- èŠ‚çœçš„æ—¶é—´è¶³å¤Ÿåšæ›´å¤šæ¶ˆèå®éªŒ

### å­˜å‚¨è§„åˆ’:

```
æ€»ç©ºé—´éœ€æ±‚: ~100GB
â”œâ”€â”€ COCOå›¾åƒ: ~20GB
â”‚   â”œâ”€â”€ train2014: 13GB
â”‚   â””â”€â”€ val2014: 6GB
â”œâ”€â”€ CLIPç‰¹å¾: ~50GB
â”‚   â”œâ”€â”€ train_vitb16.hdf5: 18GB
â”‚   â”œâ”€â”€ val_vitb16.hdf5: 2GB
â”‚   â””â”€â”€ train_vitl14.hdf5: 30GB (å¯é€‰)
â”œâ”€â”€ æ ‡æ³¨æ–‡ä»¶: ~1GB
â”œâ”€â”€ æ¨¡å‹checkpoint: ~10GB
â””â”€â”€ å…¶ä»–: ~19GB
```

## å¸¸è§é—®é¢˜

**Q: å¿…é¡»ç”¨HDF5æ ¼å¼å—?**
A: ä¸æ˜¯å¿…é¡»,ä¹Ÿå¯ä»¥ç”¨:
- PyTorch .pt æ–‡ä»¶
- NumPy .npz æ–‡ä»¶
- LMDBæ•°æ®åº“

ä½†HDF5æœ€å¸¸ç”¨,å› ä¸º:
- æ”¯æŒéšæœºè®¿é—®(ä¸ç”¨å…¨éƒ¨åŠ è½½åˆ°å†…å­˜)
- å‹ç¼©æ•ˆç‡é«˜
- è·¨å¹³å°å…¼å®¹æ€§å¥½

**Q: ç‰¹å¾æ–‡ä»¶å¯ä»¥å…±äº«å—?**
A: å¯ä»¥! å¦‚æœå®éªŒå®¤æœ‰äººå·²ç»æå–è¿‡:
- ç›´æ¥å¤åˆ¶ä½¿ç”¨
- ç¡®è®¤CLIPæ¨¡å‹ç‰ˆæœ¬ä¸€è‡´
- æ£€æŸ¥grid sizeæ˜¯å¦åŒ¹é…

**Q: å¦‚ä½•ç¡®è®¤ç‰¹å¾æ­£ç¡®?**
A: è¿è¡Œä»¥ä¸‹æµ‹è¯•:
```python
import h5py
import torch
import clip

# åŠ è½½CLIP
model, preprocess = clip.load('ViT-B/16')

# éšæœºé€‰ä¸€å¼ å›¾æµ‹è¯•
from PIL import Image
img = Image.open('path/to/image.jpg')
img_tensor = preprocess(img).unsqueeze(0)

# å®æ—¶æå–
with torch.no_grad():
    features_realtime = model.encode_image(img_tensor)

# ä»HDF5è¯»å–
f = h5py.File('features.hdf5', 'r')
features_saved = f['image_id_grids'][()]

# æ¯”è¾ƒ (åº”è¯¥éå¸¸æ¥è¿‘)
print("å·®å¼‚:", torch.abs(features_realtime - torch.tensor(features_saved)).mean())
```

å¥½äº†,ç°åœ¨ä½ æœ‰ä¸¤ä¸ªæ¸…æ™°çš„é€‰é¡¹äº†! æˆ‘å»ºè®®:**å…ˆç”¨æ–¹æ¡ˆAéªŒè¯,å†åˆ‡æ¢åˆ°æ–¹æ¡ˆBåšå®éªŒã€‚**
