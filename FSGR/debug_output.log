/root/miniconda3/envs/m2release/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
使用设备: cuda
实验参数: exp_name=fsgr_baseline_test, batch_size=32, xe_base_lr=0.0002
加载已有词汇表...
词汇表大小: 10201
Loaded text embeddings from text_embeddings/ram_ViT16_clip_text.pth
===Missing keys: ['visual.prompt_embeddings', 'visual.deep_prompt_embeddings', 'visual.transformer.resblocks.6.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.6.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.6.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.6.S_Adapter.D_fc2.bias', 'visual.transformer.resblocks.7.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.7.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.7.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.7.S_Adapter.D_fc2.bias', 'visual.transformer.resblocks.8.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.8.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.8.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.8.S_Adapter.D_fc2.bias', 'visual.transformer.resblocks.9.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.9.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.9.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.9.S_Adapter.D_fc2.bias', 'visual.transformer.resblocks.10.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.10.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.10.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.10.S_Adapter.D_fc2.bias', 'visual.transformer.resblocks.11.S_Adapter.D_fc1.weight', 'visual.transformer.resblocks.11.S_Adapter.D_fc1.bias', 'visual.transformer.resblocks.11.S_Adapter.D_fc2.weight', 'visual.transformer.resblocks.11.S_Adapter.D_fc2.bias', 'visual.prompt_proj.weight', 'visual.prompt_proj.bias', 'visual.prompt_norm.weight', 'visual.prompt_norm.bias', 'transformer.resblocks.0.S_Adapter.D_fc1.weight', 'transformer.resblocks.0.S_Adapter.D_fc1.bias', 'transformer.resblocks.0.S_Adapter.D_fc2.weight', 'transformer.resblocks.0.S_Adapter.D_fc2.bias', 'transformer.resblocks.1.S_Adapter.D_fc1.weight', 'transformer.resblocks.1.S_Adapter.D_fc1.bias', 'transformer.resblocks.1.S_Adapter.D_fc2.weight', 'transformer.resblocks.1.S_Adapter.D_fc2.bias', 'transformer.resblocks.2.S_Adapter.D_fc1.weight', 'transformer.resblocks.2.S_Adapter.D_fc1.bias', 'transformer.resblocks.2.S_Adapter.D_fc2.weight', 'transformer.resblocks.2.S_Adapter.D_fc2.bias', 'transformer.resblocks.3.S_Adapter.D_fc1.weight', 'transformer.resblocks.3.S_Adapter.D_fc1.bias', 'transformer.resblocks.3.S_Adapter.D_fc2.weight', 'transformer.resblocks.3.S_Adapter.D_fc2.bias', 'transformer.resblocks.4.S_Adapter.D_fc1.weight', 'transformer.resblocks.4.S_Adapter.D_fc1.bias', 'transformer.resblocks.4.S_Adapter.D_fc2.weight', 'transformer.resblocks.4.S_Adapter.D_fc2.bias', 'transformer.resblocks.5.S_Adapter.D_fc1.weight', 'transformer.resblocks.5.S_Adapter.D_fc1.bias', 'transformer.resblocks.5.S_Adapter.D_fc2.weight', 'transformer.resblocks.5.S_Adapter.D_fc2.bias', 'transformer.resblocks.6.S_Adapter.D_fc1.weight', 'transformer.resblocks.6.S_Adapter.D_fc1.bias', 'transformer.resblocks.6.S_Adapter.D_fc2.weight', 'transformer.resblocks.6.S_Adapter.D_fc2.bias', 'transformer.resblocks.7.S_Adapter.D_fc1.weight', 'transformer.resblocks.7.S_Adapter.D_fc1.bias', 'transformer.resblocks.7.S_Adapter.D_fc2.weight', 'transformer.resblocks.7.S_Adapter.D_fc2.bias', 'transformer.resblocks.8.S_Adapter.D_fc1.weight', 'transformer.resblocks.8.S_Adapter.D_fc1.bias', 'transformer.resblocks.8.S_Adapter.D_fc2.weight', 'transformer.resblocks.8.S_Adapter.D_fc2.bias', 'transformer.resblocks.9.S_Adapter.D_fc1.weight', 'transformer.resblocks.9.S_Adapter.D_fc1.bias', 'transformer.resblocks.9.S_Adapter.D_fc2.weight', 'transformer.resblocks.9.S_Adapter.D_fc2.bias', 'transformer.resblocks.10.S_Adapter.D_fc1.weight', 'transformer.resblocks.10.S_Adapter.D_fc1.bias', 'transformer.resblocks.10.S_Adapter.D_fc2.weight', 'transformer.resblocks.10.S_Adapter.D_fc2.bias', 'transformer.resblocks.11.S_Adapter.D_fc1.weight', 'transformer.resblocks.11.S_Adapter.D_fc1.bias', 'transformer.resblocks.11.S_Adapter.D_fc2.weight', 'transformer.resblocks.11.S_Adapter.D_fc2.bias']
===Unexpected keys: []
load pretrained weights!
开始训练...

===== Epoch 0 =====
Backbone lr = 0.000000, Dec lr = 0.000000
Epoch 0 - train:   0%|                                                                                             | 0/17710 [00:00<?, ?it/s]Epoch 0 - train:   0%|                                                                                  | 1/17710 [00:02<10:50:38,  2.20s/it]Epoch 0 - train:   0%|                                                                                   | 3/17710 [00:03<5:03:04,  1.03s/it]Epoch 0 - train:   0%|                                                                                   | 5/17710 [00:04<3:57:35,  1.24it/s]Epoch 0 - train:   0%|                                                                                   | 7/17710 [00:05<3:32:03,  1.39it/s]Epoch 0 - train:   0%|                                                                                   | 9/17710 [00:07<3:19:39,  1.48it/s]Epoch 0 - train:   0%|                                                                                  | 11/17710 [00:08<3:13:21,  1.53it/s]Epoch 0 - train:   0%|                                                                                  | 13/17710 [00:09<3:05:06,  1.59it/s]Epoch 0 - train:   0%|                                                                                  | 15/17710 [00:10<3:00:44,  1.63it/s]DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
Epoch 0 - train:   0%|                                                                                  | 17/17710 [00:11<2:55:24,  1.68it/s]Epoch 0 - train:   0%|                                                                                  | 19/17710 [00:12<2:52:03,  1.71it/s]Epoch 0 - train:   0%|                                                                                  | 21/17710 [00:13<2:48:23,  1.75it/s]Epoch 0 - train:   0%|                                                                                  | 23/17710 [00:15<2:46:46,  1.77it/s]Epoch 0 - train:   0%|                                                                                  | 25/17710 [00:16<2:45:02,  1.79it/s]Epoch 0 - train:   0%|▏                                                                                 | 27/17710 [00:17<2:44:56,  1.79it/s]Epoch 0 - train:   0%|▏                                                                                 | 29/17710 [00:18<2:43:05,  1.81it/s]mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
Epoch 0 - train:   0%|▏                                                                                 | 31/17710 [00:19<2:42:51,  1.81it/s]Epoch 0 - train:   0%|▏                                                                                 | 33/17710 [00:20<2:42:33,  1.81it/s]Epoch 0 - train:   0%|▏                                                                                 | 35/17710 [00:21<2:43:34,  1.80it/s]Epoch 0 - train:   0%|▏                                                                                 | 37/17710 [00:22<2:41:08,  1.83it/s]Epoch 0 - train:   0%|▏                                                                                 | 39/17710 [00:23<2:40:54,  1.83it/s]Epoch 0 - train:   0%|▏                                                                                 | 41/17710 [00:24<2:41:03,  1.83it/s]Epoch 0 - train:   0%|▏                                                                                 | 43/17710 [00:25<2:42:35,  1.81it/s]Epoch 0 - train:   0%|▏                                                                                 | 45/17710 [00:27<2:44:34,  1.79it/s]DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
Epoch 0 - train:   0%|▏                                                                                 | 47/17710 [00:28<2:43:59,  1.80it/s]Epoch 0 - train:   0%|▏                                                                                 | 49/17710 [00:29<2:43:49,  1.80it/s]Epoch 0 - train:   0%|▏                                                                      | 49/17710 [00:29<2:43:49,  1.80it/s, loss=9.28]Epoch 0 - train:   1%|▍                                                                       | 100/17710 [00:30<19:28, 15.07it/s, loss=9.28]DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
Epoch 0 - train:   1%|▍                                                                     | 116/17710 [00:39<1:01:02,  4.80it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
Epoch 0 - train:   1%|▌                                                                     | 128/17710 [00:45<1:24:16,  3.48it/s, loss=9.28]Epoch 0 - train:   1%|▌                                                                     | 137/17710 [00:50<1:38:34,  2.97it/s, loss=9.28]padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
Epoch 0 - train:   1%|▌                                                                     | 144/17710 [00:54<1:48:52,  2.69it/s, loss=9.28]Epoch 0 - train:   1%|▌                                                                     | 148/17710 [00:57<1:48:51,  2.69it/s, loss=9.28]Epoch 0 - train:   1%|▊                                                                       | 198/17710 [00:57<47:24,  6.16it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 37]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 37, 37])
input shape: torch.Size([32, 37])
padding check shape: torch.Size([32, 37])
After unsqueeze shape: torch.Size([32, 1, 1, 37])
DEBUG: input.shape=torch.Size([32, 37])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 37, 37])
DEBUG: (input==padding).shape=torch.Size([32, 37])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 37])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:   1%|▊                                                                     | 207/17710 [01:02<1:04:55,  4.49it/s, loss=9.28]Epoch 0 - train:   1%|▊                                                                     | 214/17710 [01:06<1:17:31,  3.76it/s, loss=9.28]Epoch 0 - train:   1%|▊                                                                     | 220/17710 [01:10<1:29:03,  3.27it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
Epoch 0 - train:   1%|▉                                                                     | 225/17710 [01:12<1:39:25,  2.93it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 229/17710 [01:15<1:48:08,  2.69it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 232/17710 [01:16<1:54:36,  2.54it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 235/17710 [01:18<2:01:53,  2.39it/s, loss=9.28]padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
Epoch 0 - train:   1%|▉                                                                     | 238/17710 [01:19<2:08:24,  2.27it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 241/17710 [01:21<2:14:35,  2.16it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 244/17710 [01:23<2:21:26,  2.06it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 247/17710 [01:24<2:27:23,  1.97it/s, loss=9.28]Epoch 0 - train:   1%|▉                                                                     | 247/17710 [01:25<2:27:23,  1.97it/s, loss=9.28]Epoch 0 - train:   2%|█▏                                                                      | 298/17710 [01:26<26:27, 10.97it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
Epoch 0 - train:   2%|█▏                                                                    | 314/17710 [01:35<1:04:13,  4.51it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
Epoch 0 - train:   2%|█▎                                                                    | 326/17710 [01:41<1:25:32,  3.39it/s, loss=9.28]padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
Epoch 0 - train:   2%|█▎                                                                    | 335/17710 [01:46<1:40:29,  2.88it/s, loss=9.28]Epoch 0 - train:   2%|█▎                                                                    | 342/17710 [01:50<1:51:08,  2.60it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
Epoch 0 - train:   2%|█▎                                                                    | 346/17710 [01:53<1:51:06,  2.60it/s, loss=9.28]Epoch 0 - train:   2%|█▌                                                                      | 396/17710 [01:53<47:04,  6.13it/s, loss=9.28]Epoch 0 - train:   2%|█▌                                                                    | 406/17710 [01:59<1:05:03,  4.43it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 35]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 35, 35])
input shape: torch.Size([32, 35])
padding check shape: torch.Size([32, 35])
After unsqueeze shape: torch.Size([32, 1, 1, 35])
DEBUG: input.shape=torch.Size([32, 35])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 35, 35])
DEBUG: (input==padding).shape=torch.Size([32, 35])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 35])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
Epoch 0 - train:   2%|█▋                                                                    | 414/17710 [02:03<1:18:34,  3.67it/s, loss=9.28]Epoch 0 - train:   2%|█▋                                                                    | 420/17710 [02:06<1:29:27,  3.22it/s, loss=9.28]Epoch 0 - train:   2%|█▋                                                                    | 425/17710 [02:09<1:39:00,  2.91it/s, loss=9.28]padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 33]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 33, 33])
input shape: torch.Size([32, 33])
padding check shape: torch.Size([32, 33])
After unsqueeze shape: torch.Size([32, 1, 1, 33])
DEBUG: input.shape=torch.Size([32, 33])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 33, 33])
DEBUG: (input==padding).shape=torch.Size([32, 33])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 33])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
Epoch 0 - train:   2%|█▋                                                                    | 429/17710 [02:11<1:47:23,  2.68it/s, loss=9.28]Epoch 0 - train:   2%|█▋                                                                    | 433/17710 [02:13<1:55:26,  2.49it/s, loss=9.28]Epoch 0 - train:   2%|█▋                                                                    | 436/17710 [02:15<2:02:00,  2.36it/s, loss=9.28]Epoch 0 - train:   2%|█▋                                                                    | 439/17710 [02:17<2:07:38,  2.26it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 35]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 35, 35])
input shape: torch.Size([32, 35])
padding check shape: torch.Size([32, 35])
After unsqueeze shape: torch.Size([32, 1, 1, 35])
DEBUG: input.shape=torch.Size([32, 35])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 35, 35])
DEBUG: (input==padding).shape=torch.Size([32, 35])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 35])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
Epoch 0 - train:   2%|█▋                                                                    | 442/17710 [02:18<2:13:18,  2.16it/s, loss=9.28]Epoch 0 - train:   3%|█▊                                                                    | 445/17710 [02:20<2:19:19,  2.07it/s, loss=9.28]Epoch 0 - train:   3%|█▊                                                                    | 445/17710 [02:20<2:19:19,  2.07it/s, loss=9.28]Epoch 0 - train:   3%|██                                                                      | 496/17710 [02:21<26:49, 10.69it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
Epoch 0 - train:   3%|██                                                                    | 512/17710 [02:30<1:03:28,  4.52it/s, loss=9.28]padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
Epoch 0 - train:   3%|██                                                                    | 524/17710 [02:37<1:24:57,  3.37it/s, loss=9.28]Epoch 0 - train:   3%|██                                                                    | 533/17710 [02:42<1:39:45,  2.87it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 32]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 32, 32])
input shape: torch.Size([32, 32])
padding check shape: torch.Size([32, 32])
After unsqueeze shape: torch.Size([32, 1, 1, 32])
DEBUG: input.shape=torch.Size([32, 32])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 32, 32])
DEBUG: (input==padding).shape=torch.Size([32, 32])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 32])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
Epoch 0 - train:   3%|██▏                                                                   | 540/17710 [02:46<1:49:59,  2.60it/s, loss=9.28]Epoch 0 - train:   3%|██▏                                                                   | 544/17710 [02:48<1:49:58,  2.60it/s, loss=9.28]Epoch 0 - train:   3%|██▍                                                                     | 594/17710 [02:48<46:43,  6.10it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
Epoch 0 - train:   3%|██▍                                                                   | 604/17710 [02:54<1:02:51,  4.54it/s, loss=9.28]Epoch 0 - train:   3%|██▍                                                                   | 612/17710 [02:58<1:18:14,  3.64it/s, loss=9.28]padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
Epoch 0 - train:   3%|██▍                                                                   | 618/17710 [03:02<1:29:35,  3.18it/s, loss=9.28]Epoch 0 - train:   4%|██▍                                                                   | 623/17710 [03:05<1:38:26,  2.89it/s, loss=9.28]Epoch 0 - train:   4%|██▍                                                                   | 627/17710 [03:07<1:45:48,  2.69it/s, loss=9.28]Epoch 0 - train:   4%|██▍                                                                   | 631/17710 [03:09<1:53:34,  2.51it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 47]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 47, 47])
input shape: torch.Size([32, 47])
padding check shape: torch.Size([32, 47])
After unsqueeze shape: torch.Size([32, 1, 1, 47])
DEBUG: input.shape=torch.Size([32, 47])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 47, 47])
DEBUG: (input==padding).shape=torch.Size([32, 47])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 47])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
Epoch 0 - train:   4%|██▌                                                                   | 634/17710 [03:11<1:59:28,  2.38it/s, loss=9.28]Epoch 0 - train:   4%|██▌                                                                   | 637/17710 [03:12<2:05:15,  2.27it/s, loss=9.28]Epoch 0 - train:   4%|██▌                                                                   | 640/17710 [03:14<2:10:52,  2.17it/s, loss=9.28]Epoch 0 - train:   4%|██▌                                                                   | 643/17710 [03:15<2:16:51,  2.08it/s, loss=9.28]Epoch 0 - train:   4%|██▌                                                                   | 643/17710 [03:16<2:16:51,  2.08it/s, loss=9.28]Epoch 0 - train:   4%|██▊                                                                     | 694/17710 [03:17<26:35, 10.66it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 31]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 31, 31])
input shape: torch.Size([32, 31])
padding check shape: torch.Size([32, 31])
After unsqueeze shape: torch.Size([32, 1, 1, 31])
DEBUG: input.shape=torch.Size([32, 31])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 31, 31])
DEBUG: (input==padding).shape=torch.Size([32, 31])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 31])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 31]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 31, 31])
input shape: torch.Size([32, 31])
padding check shape: torch.Size([32, 31])
After unsqueeze shape: torch.Size([32, 1, 1, 31])
DEBUG: input.shape=torch.Size([32, 31])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 31, 31])
DEBUG: (input==padding).shape=torch.Size([32, 31])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 31])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
Epoch 0 - train:   4%|██▊                                                                   | 709/17710 [03:25<1:00:52,  4.65it/s, loss=9.28]padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 33]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 33, 33])
input shape: torch.Size([32, 33])
padding check shape: torch.Size([32, 33])
After unsqueeze shape: torch.Size([32, 1, 1, 33])
DEBUG: input.shape=torch.Size([32, 33])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 33, 33])
DEBUG: (input==padding).shape=torch.Size([32, 33])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 33])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
Epoch 0 - train:   4%|██▊                                                                   | 721/17710 [03:32<1:22:56,  3.41it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
Epoch 0 - train:   4%|██▉                                                                   | 730/17710 [03:36<1:37:05,  2.92it/s, loss=9.28]Epoch 0 - train:   4%|██▉                                                                   | 737/17710 [03:40<1:47:10,  2.64it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
Epoch 0 - train:   4%|██▉                                                                   | 742/17710 [03:43<1:54:48,  2.46it/s, loss=9.28]Epoch 0 - train:   4%|██▉                                                                   | 742/17710 [03:44<1:54:48,  2.46it/s, loss=9.28]Epoch 0 - train:   4%|███▏                                                                    | 793/17710 [03:44<39:16,  7.18it/s, loss=9.28]padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 52]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 52, 52])
input shape: torch.Size([32, 52])
padding check shape: torch.Size([32, 52])
After unsqueeze shape: torch.Size([32, 1, 1, 52])
DEBUG: input.shape=torch.Size([32, 52])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 52, 52])
DEBUG: (input==padding).shape=torch.Size([32, 52])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 52])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
Epoch 0 - train:   5%|███▏                                                                  | 810/17710 [03:53<1:07:15,  4.19it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
Epoch 0 - train:   5%|███▎                                                                  | 823/17710 [04:01<1:27:00,  3.23it/s, loss=9.28]Epoch 0 - train:   5%|███▎                                                                  | 833/17710 [04:06<1:39:28,  2.83it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 31]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 31, 31])
input shape: torch.Size([32, 31])
padding check shape: torch.Size([32, 31])
After unsqueeze shape: torch.Size([32, 1, 1, 31])
DEBUG: input.shape=torch.Size([32, 31])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 31, 31])
DEBUG: (input==padding).shape=torch.Size([32, 31])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 31])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 48]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 48, 48])
input shape: torch.Size([32, 48])
padding check shape: torch.Size([32, 48])
After unsqueeze shape: torch.Size([32, 1, 1, 48])
DEBUG: input.shape=torch.Size([32, 48])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 48, 48])
DEBUG: (input==padding).shape=torch.Size([32, 48])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 48])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
Epoch 0 - train:   5%|███▎                                                                  | 840/17710 [04:10<1:47:41,  2.61it/s, loss=9.28]Epoch 0 - train:   5%|███▎                                                                  | 841/17710 [04:11<1:47:40,  2.61it/s, loss=9.28]Epoch 0 - train:   5%|███▌                                                                    | 891/17710 [04:11<44:11,  6.34it/s, loss=9.28]padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 51]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 51, 51])
input shape: torch.Size([32, 51])
padding check shape: torch.Size([32, 51])
After unsqueeze shape: torch.Size([32, 1, 1, 51])
DEBUG: input.shape=torch.Size([32, 51])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 51, 51])
DEBUG: (input==padding).shape=torch.Size([32, 51])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 51])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 31]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 31, 31])
input shape: torch.Size([32, 31])
padding check shape: torch.Size([32, 31])
After unsqueeze shape: torch.Size([32, 1, 1, 31])
DEBUG: input.shape=torch.Size([32, 31])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 31, 31])
DEBUG: (input==padding).shape=torch.Size([32, 31])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 31])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
Epoch 0 - train:   5%|███▌                                                                  | 909/17710 [04:21<1:10:27,  3.97it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:   5%|███▋                                                                  | 922/17710 [04:28<1:26:56,  3.22it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
Epoch 0 - train:   5%|███▋                                                                  | 932/17710 [04:34<1:38:53,  2.83it/s, loss=9.28]Epoch 0 - train:   5%|███▋                                                                  | 940/17710 [04:38<1:47:52,  2.59it/s, loss=9.28]Epoch 0 - train:   5%|███▋                                                                  | 940/17710 [04:38<1:47:52,  2.59it/s, loss=9.28]Epoch 0 - train:   6%|████                                                                    | 991/17710 [04:39<45:49,  6.08it/s, loss=9.28]padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 38]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 38, 38])
input shape: torch.Size([32, 38])
padding check shape: torch.Size([32, 38])
After unsqueeze shape: torch.Size([32, 1, 1, 38])
DEBUG: input.shape=torch.Size([32, 38])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 38, 38])
DEBUG: (input==padding).shape=torch.Size([32, 38])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 38])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
Epoch 0 - train:   6%|███▉                                                                 | 1010/17710 [04:49<1:11:10,  3.91it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 38]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 38, 38])
input shape: torch.Size([32, 38])
padding check shape: torch.Size([32, 38])
After unsqueeze shape: torch.Size([32, 1, 1, 38])
DEBUG: input.shape=torch.Size([32, 38])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 38, 38])
DEBUG: (input==padding).shape=torch.Size([32, 38])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 38])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
Epoch 0 - train:   6%|███▉                                                                 | 1012/17710 [04:50<1:13:50,  3.77it/s, loss=9.28]Epoch 0 - train:   6%|███▉                                                                 | 1026/17710 [04:59<1:40:36,  2.76it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 44]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 44, 44])
input shape: torch.Size([32, 44])
padding check shape: torch.Size([32, 44])
After unsqueeze shape: torch.Size([32, 1, 1, 44])
DEBUG: input.shape=torch.Size([32, 44])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 44, 44])
DEBUG: (input==padding).shape=torch.Size([32, 44])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 44])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
Epoch 0 - train:   6%|████                                                                 | 1036/17710 [05:05<1:52:36,  2.47it/s, loss=9.28]Epoch 0 - train:   6%|████                                                                 | 1039/17710 [05:07<1:52:34,  2.47it/s, loss=9.28]Epoch 0 - train:   6%|████▎                                                                  | 1089/17710 [05:07<50:10,  5.52it/s, loss=9.28]padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
Epoch 0 - train:   6%|████▎                                                                | 1102/17710 [05:14<1:07:46,  4.08it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 46]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 46, 46])
input shape: torch.Size([32, 46])
padding check shape: torch.Size([32, 46])
After unsqueeze shape: torch.Size([32, 1, 1, 46])
DEBUG: input.shape=torch.Size([32, 46])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 46, 46])
DEBUG: (input==padding).shape=torch.Size([32, 46])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 46])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:   6%|████▎                                                                | 1112/17710 [05:19<1:21:20,  3.40it/s, loss=9.28]Epoch 0 - train:   6%|████▎                                                                | 1120/17710 [05:24<1:32:58,  2.97it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
Epoch 0 - train:   6%|████▍                                                                | 1126/17710 [05:27<1:42:20,  2.70it/s, loss=9.28]Epoch 0 - train:   6%|████▍                                                                | 1131/17710 [05:30<1:49:51,  2.52it/s, loss=9.28]Epoch 0 - train:   6%|████▍                                                                | 1135/17710 [05:32<1:56:02,  2.38it/s, loss=9.28]padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
Epoch 0 - train:   6%|████▍                                                                | 1138/17710 [05:35<1:56:01,  2.38it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                  | 1188/17710 [05:35<39:19,  7.00it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                  | 1198/17710 [05:40<57:32,  4.78it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:   7%|████▋                                                                | 1206/17710 [05:44<1:12:04,  3.82it/s, loss=9.28]Epoch 0 - train:   7%|████▋                                                                | 1212/17710 [05:48<1:23:29,  3.29it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 32]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 32, 32])
input shape: torch.Size([32, 32])
Epoch 0 - train:   7%|████▋                                                                | 1217/17710 [05:50<1:33:12,  2.95it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1221/17710 [05:53<1:41:40,  2.70it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1225/17710 [05:55<1:49:52,  2.50it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1228/17710 [06:00<2:41:25,  1.70it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1231/17710 [06:02<2:46:51,  1.65it/s, loss=9.28]padding check shape: torch.Size([32, 32])
After unsqueeze shape: torch.Size([32, 1, 1, 32])
DEBUG: input.shape=torch.Size([32, 32])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 32, 32])
DEBUG: (input==padding).shape=torch.Size([32, 32])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 32])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
Epoch 0 - train:   7%|████▊                                                                | 1233/17710 [06:03<2:44:56,  1.66it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1235/17710 [06:04<2:41:36,  1.70it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1237/17710 [06:05<2:38:36,  1.73it/s, loss=9.28]Epoch 0 - train:   7%|████▊                                                                | 1237/17710 [06:06<2:38:36,  1.73it/s, loss=9.28]Epoch 0 - train:   7%|█████▏                                                                 | 1288/17710 [06:06<24:25, 11.21it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 36]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 36, 36])
input shape: torch.Size([32, 36])
padding check shape: torch.Size([32, 36])
After unsqueeze shape: torch.Size([32, 1, 1, 36])
DEBUG: input.shape=torch.Size([32, 36])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 36, 36])
DEBUG: (input==padding).shape=torch.Size([32, 36])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 36])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
Epoch 0 - train:   7%|█████▏                                                                 | 1304/17710 [06:15<59:17,  4.61it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
Epoch 0 - train:   7%|█████▏                                                               | 1316/17710 [06:21<1:20:01,  3.41it/s, loss=9.28]Epoch 0 - train:   7%|█████▏                                                               | 1325/17710 [06:26<1:34:04,  2.90it/s, loss=9.28]padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
Epoch 0 - train:   8%|█████▏                                                               | 1332/17710 [06:30<1:44:37,  2.61it/s, loss=9.28]Epoch 0 - train:   8%|█████▏                                                               | 1336/17710 [06:33<1:44:35,  2.61it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                                 | 1386/17710 [06:33<44:08,  6.16it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:   8%|█████▍                                                               | 1396/17710 [06:38<1:00:00,  4.53it/s, loss=9.28]Epoch 0 - train:   8%|█████▍                                                               | 1404/17710 [06:43<1:13:21,  3.71it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 33]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 33, 33])
input shape: torch.Size([32, 33])
padding check shape: torch.Size([32, 33])
After unsqueeze shape: torch.Size([32, 1, 1, 33])
DEBUG: input.shape=torch.Size([32, 33])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 33, 33])
DEBUG: (input==padding).shape=torch.Size([32, 33])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 33])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 49]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 49, 49])
input shape: torch.Size([32, 49])
padding check shape: torch.Size([32, 49])
After unsqueeze shape: torch.Size([32, 1, 1, 49])
DEBUG: input.shape=torch.Size([32, 49])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 49, 49])
DEBUG: (input==padding).shape=torch.Size([32, 49])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 49])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
Epoch 0 - train:   8%|█████▍                                                               | 1410/17710 [06:46<1:23:32,  3.25it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1415/17710 [06:49<1:32:37,  2.93it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1419/17710 [06:51<1:40:09,  2.71it/s, loss=9.28]padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 49]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 49, 49])
input shape: torch.Size([32, 49])
padding check shape: torch.Size([32, 49])
After unsqueeze shape: torch.Size([32, 1, 1, 49])
DEBUG: input.shape=torch.Size([32, 49])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 49, 49])
DEBUG: (input==padding).shape=torch.Size([32, 49])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 49])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
Epoch 0 - train:   8%|█████▌                                                               | 1423/17710 [06:53<1:47:34,  2.52it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1426/17710 [06:55<1:52:59,  2.40it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1429/17710 [06:57<2:01:20,  2.24it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1432/17710 [06:58<2:06:35,  2.14it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1435/17710 [07:00<2:12:30,  2.05it/s, loss=9.28]Epoch 0 - train:   8%|█████▌                                                               | 1435/17710 [07:01<2:12:30,  2.05it/s, loss=9.28]Epoch 0 - train:   8%|█████▉                                                                 | 1486/17710 [07:01<25:31, 10.59it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 35]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 35, 35])
input shape: torch.Size([32, 35])
padding check shape: torch.Size([32, 35])
After unsqueeze shape: torch.Size([32, 1, 1, 35])
DEBUG: input.shape=torch.Size([32, 35])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 35, 35])
DEBUG: (input==padding).shape=torch.Size([32, 35])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 35])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
Epoch 0 - train:   8%|██████                                                                 | 1502/17710 [07:10<58:27,  4.62it/s, loss=9.28]Epoch 0 - train:   9%|█████▉                                                               | 1514/17710 [07:16<1:18:08,  3.45it/s, loss=9.28]padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 29]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 29, 29])
input shape: torch.Size([32, 29])
padding check shape: torch.Size([32, 29])
After unsqueeze shape: torch.Size([32, 1, 1, 29])
DEBUG: input.shape=torch.Size([32, 29])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 29, 29])
DEBUG: (input==padding).shape=torch.Size([32, 29])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 29])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 41]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 41, 41])
input shape: torch.Size([32, 41])
padding check shape: torch.Size([32, 41])
After unsqueeze shape: torch.Size([32, 1, 1, 41])
DEBUG: input.shape=torch.Size([32, 41])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 41, 41])
Epoch 0 - train:   9%|█████▉                                                               | 1523/17710 [07:21<1:31:50,  2.94it/s, loss=9.28]Epoch 0 - train:   9%|█████▉                                                               | 1530/17710 [07:25<1:41:45,  2.65it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 41])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 41])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
Epoch 0 - train:   9%|█████▉                                                               | 1534/17710 [07:28<1:41:43,  2.65it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                                | 1584/17710 [07:28<43:26,  6.19it/s, loss=9.28]Epoch 0 - train:   9%|██████▍                                                                | 1594/17710 [07:33<58:30,  4.59it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
Epoch 0 - train:   9%|██████▏                                                              | 1602/17710 [07:37<1:10:58,  3.78it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1608/17710 [07:41<1:20:58,  3.31it/s, loss=9.28]padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 33]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 33, 33])
input shape: torch.Size([32, 33])
padding check shape: torch.Size([32, 33])
After unsqueeze shape: torch.Size([32, 1, 1, 33])
DEBUG: input.shape=torch.Size([32, 33])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 33, 33])
DEBUG: (input==padding).shape=torch.Size([32, 33])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 33])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
Epoch 0 - train:   9%|██████▎                                                              | 1613/17710 [07:43<1:29:49,  2.99it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1617/17710 [07:46<1:37:24,  2.75it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1621/17710 [07:48<1:45:18,  2.55it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1624/17710 [07:49<1:51:19,  2.41it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1627/17710 [07:51<1:57:15,  2.29it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
Epoch 0 - train:   9%|██████▎                                                              | 1630/17710 [07:53<2:02:41,  2.18it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1633/17710 [07:54<2:07:24,  2.10it/s, loss=9.28]Epoch 0 - train:   9%|██████▎                                                              | 1633/17710 [07:55<2:07:24,  2.10it/s, loss=9.28]Epoch 0 - train:  10%|██████▊                                                                | 1684/17710 [07:55<24:27, 10.92it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 38]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 38, 38])
input shape: torch.Size([32, 38])
padding check shape: torch.Size([32, 38])
After unsqueeze shape: torch.Size([32, 1, 1, 38])
DEBUG: input.shape=torch.Size([32, 38])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 38, 38])
DEBUG: (input==padding).shape=torch.Size([32, 38])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 38])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
Epoch 0 - train:  10%|██████▋                                                              | 1701/17710 [08:05<1:00:50,  4.39it/s, loss=9.28]padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
Epoch 0 - train:  10%|██████▋                                                              | 1713/17710 [08:11<1:19:26,  3.36it/s, loss=9.28]Epoch 0 - train:  10%|██████▋                                                              | 1722/17710 [08:16<1:32:27,  2.88it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
Epoch 0 - train:  10%|██████▋                                                              | 1729/17710 [08:20<1:41:17,  2.63it/s, loss=9.28]Epoch 0 - train:  10%|██████▋                                                              | 1732/17710 [08:22<1:41:16,  2.63it/s, loss=9.28]Epoch 0 - train:  10%|███████▏                                                               | 1782/17710 [08:22<41:41,  6.37it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
Epoch 0 - train:  10%|███████▏                                                               | 1794/17710 [08:29<59:53,  4.43it/s, loss=9.28]padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
Epoch 0 - train:  10%|███████                                                              | 1803/17710 [08:34<1:12:56,  3.63it/s, loss=9.28]Epoch 0 - train:  10%|███████                                                              | 1810/17710 [08:37<1:23:48,  3.16it/s, loss=9.28]Epoch 0 - train:  10%|███████                                                              | 1816/17710 [08:41<1:32:42,  2.86it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 33]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 33, 33])
input shape: torch.Size([32, 33])
padding check shape: torch.Size([32, 33])
After unsqueeze shape: torch.Size([32, 1, 1, 33])
DEBUG: input.shape=torch.Size([32, 33])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 33, 33])
DEBUG: (input==padding).shape=torch.Size([32, 33])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 33])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 51]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 51, 51])
input shape: torch.Size([32, 51])
padding check shape: torch.Size([32, 51])
After unsqueeze shape: torch.Size([32, 1, 1, 51])
DEBUG: input.shape=torch.Size([32, 51])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 51, 51])
DEBUG: (input==padding).shape=torch.Size([32, 51])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 51])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
Epoch 0 - train:  10%|███████                                                              | 1821/17710 [08:43<1:40:41,  2.63it/s, loss=9.28]Epoch 0 - train:  10%|███████                                                              | 1825/17710 [08:46<1:47:03,  2.47it/s, loss=9.28]Epoch 0 - train:  10%|███████                                                              | 1828/17710 [08:47<1:52:15,  2.36it/s, loss=9.28]Epoch 0 - train:  10%|███████▏                                                             | 1831/17710 [08:49<1:57:12,  2.26it/s, loss=9.28]Epoch 0 - train:  10%|███████▏                                                             | 1831/17710 [08:49<1:57:12,  2.26it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
Epoch 0 - train:  11%|███████▌                                                               | 1882/17710 [08:50<27:42,  9.52it/s, loss=9.28]padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
Epoch 0 - train:  11%|███████▌                                                               | 1898/17710 [08:59<58:00,  4.54it/s, loss=9.28]Epoch 0 - train:  11%|███████▍                                                             | 1910/17710 [09:06<1:19:48,  3.30it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 14]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 14, 14])
input shape: torch.Size([32, 14])
padding check shape: torch.Size([32, 14])
After unsqueeze shape: torch.Size([32, 1, 1, 14])
DEBUG: input.shape=torch.Size([32, 14])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 14, 14])
DEBUG: (input==padding).shape=torch.Size([32, 14])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 14])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
Epoch 0 - train:  11%|███████▍                                                             | 1919/17710 [09:11<1:33:05,  2.83it/s, loss=9.28]Epoch 0 - train:  11%|███████▌                                                             | 1926/17710 [09:15<1:42:31,  2.57it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 27]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 27, 27])
input shape: torch.Size([32, 27])
padding check shape: torch.Size([32, 27])
After unsqueeze shape: torch.Size([32, 1, 1, 27])
DEBUG: input.shape=torch.Size([32, 27])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 27, 27])
DEBUG: (input==padding).shape=torch.Size([32, 27])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 27])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
Epoch 0 - train:  11%|███████▌                                                             | 1930/17710 [09:18<1:42:29,  2.57it/s, loss=9.28]Epoch 0 - train:  11%|███████▉                                                               | 1980/17710 [09:18<43:58,  5.96it/s, loss=9.28]Epoch 0 - train:  11%|███████▉                                                               | 1990/17710 [09:23<59:15,  4.42it/s, loss=9.28]padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
Epoch 0 - train:  11%|███████▊                                                             | 1998/17710 [09:28<1:11:43,  3.65it/s, loss=9.28]Epoch 0 - train:  11%|███████▊                                                             | 2004/17710 [09:31<1:21:13,  3.22it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 34]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 34, 34])
input shape: torch.Size([32, 34])
padding check shape: torch.Size([32, 34])
After unsqueeze shape: torch.Size([32, 1, 1, 34])
DEBUG: input.shape=torch.Size([32, 34])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 34, 34])
DEBUG: (input==padding).shape=torch.Size([32, 34])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 34])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
Epoch 0 - train:  11%|███████▊                                                             | 2009/17710 [09:34<1:29:56,  2.91it/s, loss=9.28]Epoch 0 - train:  11%|███████▊                                                             | 2013/17710 [09:36<1:37:10,  2.69it/s, loss=9.28]Epoch 0 - train:  11%|███████▊                                                             | 2017/17710 [09:38<1:44:44,  2.50it/s, loss=9.28]Epoch 0 - train:  11%|███████▊                                                             | 2020/17710 [09:40<1:50:27,  2.37it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 30]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 30, 30])
input shape: torch.Size([32, 30])
Epoch 0 - train:  11%|███████▉                                                             | 2023/17710 [09:41<1:55:41,  2.26it/s, loss=9.28]Epoch 0 - train:  11%|███████▉                                                             | 2026/17710 [09:43<2:00:57,  2.16it/s, loss=9.28]Epoch 0 - train:  11%|███████▉                                                             | 2029/17710 [09:45<2:05:20,  2.09it/s, loss=9.28]Epoch 0 - train:  11%|███████▉                                                             | 2029/17710 [09:45<2:05:20,  2.09it/s, loss=9.28]Epoch 0 - train:  12%|████████▎                                                              | 2080/17710 [09:46<24:14, 10.74it/s, loss=9.28]padding check shape: torch.Size([32, 30])
After unsqueeze shape: torch.Size([32, 1, 1, 30])
DEBUG: input.shape=torch.Size([32, 30])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 30, 30])
DEBUG: (input==padding).shape=torch.Size([32, 30])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 30])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
Epoch 0 - train:  12%|████████▍                                                              | 2096/17710 [09:54<56:28,  4.61it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
Epoch 0 - train:  12%|████████▏                                                            | 2108/17710 [10:01<1:16:49,  3.38it/s, loss=9.28]Epoch 0 - train:  12%|████████▏                                                            | 2117/17710 [10:06<1:30:26,  2.87it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
Epoch 0 - train:  12%|████████▎                                                            | 2124/17710 [10:10<1:40:20,  2.59it/s, loss=9.28]Epoch 0 - train:  12%|████████▎                                                            | 2128/17710 [10:13<1:40:18,  2.59it/s, loss=9.28]Epoch 0 - train:  12%|████████▋                                                              | 2178/17710 [10:13<42:59,  6.02it/s, loss=9.28]padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 28]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 28, 28])
input shape: torch.Size([32, 28])
padding check shape: torch.Size([32, 28])
After unsqueeze shape: torch.Size([32, 1, 1, 28])
DEBUG: input.shape=torch.Size([32, 28])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 28, 28])
DEBUG: (input==padding).shape=torch.Size([32, 28])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 28])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
Epoch 0 - train:  12%|████████▊                                                              | 2188/17710 [10:19<57:44,  4.48it/s, loss=9.28]Epoch 0 - train:  12%|████████▌                                                            | 2195/17710 [10:22<1:08:45,  3.76it/s, loss=9.28]DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 25]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 25, 25])
input shape: torch.Size([32, 25])
padding check shape: torch.Size([32, 25])
After unsqueeze shape: torch.Size([32, 1, 1, 25])
DEBUG: input.shape=torch.Size([32, 25])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 25, 25])
DEBUG: (input==padding).shape=torch.Size([32, 25])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 25])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 36]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 36, 36])
input shape: torch.Size([32, 36])
padding check shape: torch.Size([32, 36])
After unsqueeze shape: torch.Size([32, 1, 1, 36])
DEBUG: input.shape=torch.Size([32, 36])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 36, 36])
DEBUG: (input==padding).shape=torch.Size([32, 36])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 36])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 24]), seq.dtype=torch.int64
Epoch 0 - train:  12%|████████▌                                                            | 2201/17710 [10:26<1:19:02,  3.27it/s, loss=9.28]Epoch 0 - train:  12%|████████▌                                                            | 2206/17710 [10:29<1:27:51,  2.94it/s, loss=9.28]Epoch 0 - train:  12%|████████▌                                                            | 2210/17710 [10:31<1:34:47,  2.73it/s, loss=9.28]DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 24, 24])
input shape: torch.Size([32, 24])
padding check shape: torch.Size([32, 24])
After unsqueeze shape: torch.Size([32, 1, 1, 24])
DEBUG: input.shape=torch.Size([32, 24])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 24, 24])
DEBUG: (input==padding).shape=torch.Size([32, 24])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 24])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 26]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 26, 26])
input shape: torch.Size([32, 26])
padding check shape: torch.Size([32, 26])
After unsqueeze shape: torch.Size([32, 1, 1, 26])
DEBUG: input.shape=torch.Size([32, 26])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 26, 26])
DEBUG: (input==padding).shape=torch.Size([32, 26])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 26])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 23]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 23, 23])
input shape: torch.Size([32, 23])
padding check shape: torch.Size([32, 23])
After unsqueeze shape: torch.Size([32, 1, 1, 23])
DEBUG: input.shape=torch.Size([32, 23])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 23, 23])
DEBUG: (input==padding).shape=torch.Size([32, 23])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 23])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 22]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 22, 22])
input shape: torch.Size([32, 22])
padding check shape: torch.Size([32, 22])
After unsqueeze shape: torch.Size([32, 1, 1, 22])
DEBUG: input.shape=torch.Size([32, 22])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 22, 22])
DEBUG: (input==padding).shape=torch.Size([32, 22])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 22])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 20]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 20, 20])
input shape: torch.Size([32, 20])
padding check shape: torch.Size([32, 20])
After unsqueeze shape: torch.Size([32, 1, 1, 20])
DEBUG: input.shape=torch.Size([32, 20])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 20, 20])
DEBUG: (input==padding).shape=torch.Size([32, 20])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 20])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 19]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 19, 19])
input shape: torch.Size([32, 19])
padding check shape: torch.Size([32, 19])
After unsqueeze shape: torch.Size([32, 1, 1, 19])
DEBUG: input.shape=torch.Size([32, 19])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 19, 19])
DEBUG: (input==padding).shape=torch.Size([32, 19])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 19])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 15]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 15, 15])
input shape: torch.Size([32, 15])
padding check shape: torch.Size([32, 15])
After unsqueeze shape: torch.Size([32, 1, 1, 15])
DEBUG: input.shape=torch.Size([32, 15])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 15, 15])
DEBUG: (input==padding).shape=torch.Size([32, 15])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 15])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 17]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 17, 17])
input shape: torch.Size([32, 17])
padding check shape: torch.Size([32, 17])
After unsqueeze shape: torch.Size([32, 1, 1, 17])
DEBUG: input.shape=torch.Size([32, 17])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 17, 17])
DEBUG: (input==padding).shape=torch.Size([32, 17])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 17])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 16]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 16, 16])
input shape: torch.Size([32, 16])
padding check shape: torch.Size([32, 16])
After unsqueeze shape: torch.Size([32, 1, 1, 16])
DEBUG: input.shape=torch.Size([32, 16])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 16, 16])
DEBUG: (input==padding).shape=torch.Size([32, 16])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 16])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 21]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 21, 21])
input shape: torch.Size([32, 21])
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [324,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [325,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [326,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [327,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
Epoch 0 - train:  12%|████████▌                                                            | 2213/17710 [10:33<1:13:54,  3.49it/s, loss=9.28]
padding check shape: torch.Size([32, 21])
After unsqueeze shape: torch.Size([32, 1, 1, 21])
DEBUG: input.shape=torch.Size([32, 21])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 21, 21])
DEBUG: (input==padding).shape=torch.Size([32, 21])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 21])
DEBUG Transformer.forward: seq.shape=torch.Size([32, 18]), seq.dtype=torch.int64
DEBUG Transformer.forward: enc_output.shape=torch.Size([32, 196, 512])
mask_self_attention shape: torch.Size([1, 1, 18, 18])
input shape: torch.Size([32, 18])
padding check shape: torch.Size([32, 18])
After unsqueeze shape: torch.Size([32, 1, 1, 18])
DEBUG: input.shape=torch.Size([32, 18])
DEBUG: mask_self_attention.shape=torch.Size([1, 1, 18, 18])
DEBUG: (input==padding).shape=torch.Size([32, 18])
DEBUG: after unsqueeze=torch.Size([32, 1, 1, 18])
Traceback (most recent call last):
  File "train_transformer_debug.py", line 385, in <module>
    train_loss = train_xe(model, dataloader_train, optim, text_field, device, loss_contrast, e, beta=args.beta)
  File "train_transformer_debug.py", line 141, in train_xe
    scaler.scale(loss).backward()
  File "/root/miniconda3/envs/m2release/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 170, in scale
    return outputs * self._scale.to(device=outputs.device, non_blocking=True)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

